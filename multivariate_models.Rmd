---
title: "Multivariate Linear Models"
subtitle: "Statistical Rethinking<br>Chapter 4"
author: "Chuck Lanfear"
date: "`r gsub(' 0', ' ', format(Sys.Date(), format='%b %d, %Y'))`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: tomorrow-night-bright
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["center","top"]
---



```{r, include=FALSE}
knitr::purl("./multivariate_models.Rmd")
```



---
# Intuition

What is the predictive value of a variable, once I already know all of the other predictor variables?

Purposes:

1. **Controlling for confounders**

2. **Capturing multiple causation**

3. Measuring interactions (next week)

---
# A multivariate model

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_diagram.PNG" height=250 class="image-center">

What do we measure?

* $\beta_1$ - slope relating $x$ to $y$ holding $z$ constant ( $\frac{\delta y}{\delta x}$ ).
* $\beta_2$ - slope relating $z$ to $y$ holding $z$ constant ( $\frac{\delta y}{\delta z}$ ).
* Implicit: $\rho$ - correlation between $x$ and $z$

---
## Spuriousness

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_spuriousness.PNG" height=250 class="image-center">



* $X$ is correlated with $Y$ mainly or only through $Z$
* If $Z$ is omitted, it appears $X$ impacts $Y$
* If $Z$ is included, estimated effect of $X$ on $Y$ is attenuated.

*This is classic omitted variable bias.*

---
## Multiple Causation

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_masquerade.PNG" height=250 class="image-center">

* $X$ directly impacts $Y$ positively.
* $Z$ directly impacts $Y$ negatively.
* $X$ and $Z$ are correlated.
* If only $X$ or $Z$ is included, its effect is attenuated (suppressed).


*This is also omitted variable bias.*

---
## Multicollinearity

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_collinearity.PNG" height=250 class="image-center">

* If two variables provide similar information, they are redundant and their effects difficult to separate.
    * Redundancy doesn't hurt prediction--you get accurate posteriors.
    * Redundancy hurts inference--you get large standard errors.

---
## Post-Treatment Bias

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_posttreatment.PNG" height=250 class="image-center">

---
# Optimizer Example

119 iterations of a Nelder-Mead optimizer on OLS.

Start Values: $\alpha = 1$, $\beta = 1$

Squares depict **squared errors**.

Minimizing the **sum squared errors** is the objective of the mean loss function in a linear regression.

<img src="http://www.jessefagan.com/content/images/2019/10/advertising_album_sales_regression_animation_06.gif">

---