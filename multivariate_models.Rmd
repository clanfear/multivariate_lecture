---
title: "Multivariate Linear Models"
subtitle: "Statistical Rethinking<br>Chapter 4"
author: "Chuck Lanfear"
date: "`r gsub(' 0', ' ', format(Sys.Date(), format='%b %d, %Y'))`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: tomorrow-night-bright
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["center","top"]
---



```{r, include=FALSE}
knitr::purl("./multivariate_models.Rmd")
```

# Setup

This brief lecture will use the `foxes` data from the Chapter 5 problem set of `rethinking`.

We'll mostly use syntax from the book, except a `dplyr` command or two for convenience.

You can follow along with purled code from the slides found here.

Packages Used:

* `rethinking`

* `dplyr`

---
# Multivariate Intuition

What is the predictive value of a variable, once I already know all of the other predictor variables?

Motivations:

1. **Controlling for confounders**

2. **Capturing multiple causation**

3. Measuring interactions (next week)

---
# A Multivariate Model

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_diagram.PNG" height=250 class="image-center">

What do we measure?

* $\beta_1$ - slope relating $x$ to $y$ holding $z$ constant ( $\frac{\delta y}{\delta x}$ ).
* $\beta_2$ - slope relating $z$ to $y$ holding $z$ constant ( $\frac{\delta y}{\delta z}$ ).
* Implicit: $\rho$ - correlation between $x$ and $z$

---
## Fitting a Model

```{r, warning=FALSE, message=FALSE}
library(dplyr); library(rethinking);  data(foxes)
foxes_std <- mutate_at(foxes, vars(-group), ~ scale(.))
m_1 <- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu <- a + b1*area + b2*groupsize + b3*avgfood, #<<
    a ~ dnorm( 0, 100),
    c(b1, b2, b3) ~ dnorm(0, 2), #<<
    sigma ~ dunif(0, 10)
  ), data=foxes_std
)
precis( m_1)
```

---
## Well, *that* doesn't look good.

```{r, fig.width=8, fig.height=4, fig.align="center"}
plot(precis(m_1))
```

*Let's talk about what can go wrong.*

---
## Spuriousness

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_spuriousness.PNG" height=250 class="image-center">

Conditions of spuriousness:

* $X$ is correlated with $Y$ mainly or only through $Z$
* If $Z$ is omitted, it appears $X$ impacts $Y$
* If $Z$ is included, estimated effect of $X$ on $Y$ is attenuated.

*This is classic omitted variable bias.*

---
## Diagnosing Spuriousness

Univariate relationship, no multivariate

---
## Multiple Causation

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_masquerade.PNG" height=250 class="image-center">

Conditions of multiple causation (suppression):

* $X$ directly impacts $Y$ positively.
* $Z$ directly impacts $Y$ negatively.
* $X$ and $Z$ are correlated.
* If only $X$ or $Z$ is included, its effect is attenuated (suppressed).

*This is also omitted variable bias.*

---
## Diagnosing Suppression 1

.small[
.pull-left[
```{r, fig.width=4, fig.height=3, fig.align="center"}
m_sp_1 <- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu <- a + b1*area, 
    a ~ dnorm( 0, 100),
    b1 ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std
)
precis(m_sp_1)
```

We see no notable relationship between area and fox weight.

]
.pull-right[
```{r, fig.width=4, fig.height=3, fig.align="center"}
m_sp_2 <- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu <- a + b2*groupsize, 
    a ~ dnorm( 0, 100),
    b2 ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std
)
precis(m_sp_2)
```

We also see nothing for group size and fox weight.

]
]

---
## Diagnosing Suppression 1

.smallish[
```{r, fig.width=6, fig.height=3, fig.align="center"}
m_sp_3 <- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu <- a + b1*area + b2*groupsize, 
    a ~ dnorm( 0, 100),
    c(b1, b2) ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std
)
plot(precis(m_sp_3))
```
]

* Net of area, group size has a negative association with weight.
* Net of group size, area has a positive assocation with weight.
* Area and group size are positively correlated.

*These variables suppress each other.*

---
## Multicollinearity

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_collinearity.PNG" height=250 class="image-center">

Conditions of multicollinearity:

* $X$ and $Z$ are highly correlated--they provide the same information.
* If $X$ and $Z$ are both included, estimates are uncertain.
* Redundancy doesn't hurt prediction--you get accurate posteriors.
* Redundancy hurts inference--you get large standard errors.

*This is not bias, but weak identification.*

---
## Diagnosing Multicollinearity

```{r, fig.width=6, fig.height=3, fig.align="center"}
m_mc_1 <- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu <- a + b1*avgfood + b2*area, 
    a ~ dnorm( 0, 100),
    c(b1, b2) ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std
)
plot(precis(m_mc_1))
cor(foxes_std)["avgfood", "area"]
```

---

.small[
.pull-left[
```{r, fig.width=4, fig.height=3}
m_mc_2 <- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu <- a + b1*area, 
    a ~ dnorm( 0, 100),
    b1 ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std
)
plot(precis(m_mc_2))
```

]

.pull-right[
```{r, fig.width=4, fig.height=3}
m_mc_3 <- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu <- a + b2*avgfood, 
    a ~ dnorm( 0, 100),
    b2 ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std
)
plot(precis(m_mc_3))
```

]
]

---
## Post-Treatment Bias

<img src="https://clanfear.github.io/multivariate_lecture/img/xzy_posttreatment.PNG" height=250 class="image-center">

Conditions of post-treatment bias:

* $X$ impacts $Y$ through $Z$--it is a mediator.
* $Z$ provides much of same information as $X$.
* If $Z$ is included, $B_2$ absorbs some of $B_1$.

*This is included variable bias.*

---
## Diagnosing Post-Treatment Bias

```{r}

```


---
# Key Takeaway

Aside from multicollinearity, it is hard to distinguish between different relationships.

Need a clear causal model for relationships to make sense of it.


---
# An Optimizer Example

119 iterations of a Nelder-Mead optimizer on OLS.

Start Values: $\alpha = 1$, $\beta = 1$

Squares depict **squared errors**.

Minimizing the **sum squared errors** is the objective of the mean loss function in a linear regression.

<img src="http://www.jessefagan.com/content/images/2019/10/advertising_album_sales_regression_animation_06.gif">

Source: Jesse Fagan, [R code available here](https://gist.github.com/jfaganUK/0f38d414d9a132d029c366ac310a2a06).

---