<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Multivariate Linear Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Chuck Lanfear" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, top, title-slide

# Multivariate Linear Models
## Statistical Rethinking<br>Chapter 5
### Chuck Lanfear
### Oct 18, 2019

---






# Setup

This brief lecture will use the `foxes` data from the Chapter 5 problem set of `rethinking`.

We'll mostly use syntax from the book, except a `dplyr` command or two for convenience.

You can follow along with purled code from the slides [found here](https://github.com/clanfear/multivariate_lecture/blob/master/multivariate_models.R).

Packages Used:

* `rethinking`

* `dplyr`

---
# Multivariate Intuition

What is the predictive value of a variable, once I already know all of the other predictor variables?

Motivations:

1. **Controlling for confounders**

2. **Capturing multiple causation**

3. Measuring interactions (next week)

---
# A Multivariate Model

&lt;img src="https://clanfear.github.io/multivariate_lecture/img/xzy_diagram.PNG" height=250 class="image-center"&gt;

What do we measure?

* `\(\beta_1\)` - slope relating `\(x\)` to `\(y\)` holding `\(z\)` constant ( `\(\frac{\delta y}{\delta x}\)` ).
* `\(\beta_2\)` - slope relating `\(z\)` to `\(y\)` holding `\(z\)` constant ( `\(\frac{\delta y}{\delta z}\)` ).
* Implicit: `\(\rho\)` - correlation between `\(x\)` and `\(z\)`

---
## Fitting a Model


```r
library(dplyr); library(rethinking); set.seed(7); data(foxes)
foxes_std &lt;- mutate_at(foxes, vars(-group), ~ scale(.))
m_1 &lt;- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
*   mu &lt;- a + b1*area + b2*groupsize + b3*avgfood,
    a             ~ dnorm( 0, 100),
*   c(b1, b2, b3) ~ dnorm(0, 2),
    sigma         ~ dunif(0, 10)
  ), data=foxes_std
)
precis( m_1)
```

```
##        Mean StdDev  5.5% 94.5%
## a      0.00   0.09 -0.14  0.14
## b1     0.30   0.19  0.00  0.60
## b2    -0.78   0.20 -1.10 -0.46
## b3     0.41   0.24  0.03  0.79
## sigma  0.93   0.06  0.83  1.03
```

---
## Well, *that* doesn't look good.


```r
plot(precis(m_1))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

*Let's talk about what can go wrong.*

---
## Spuriousness

&lt;img src="https://clanfear.github.io/multivariate_lecture/img/xzy_spuriousness.PNG" height=250 class="image-center"&gt;

Conditions of spuriousness:

* `\(X\)` is correlated with `\(Y\)` mainly or only through `\(Z\)`
* If `\(Z\)` is omitted, it appears `\(X\)` impacts `\(Y\)`
* If `\(Z\)` is included, estimated effect of `\(X\)` on `\(Y\)` is attenuated.

*This is classic omitted variable bias.*

---
## Spurious Models 1

.small[

```r
foxes_std &lt;- foxes_std %&gt;% mutate(., spur_y = rnorm(nrow(.), weight, 1),
                                     spur_x = rnorm(nrow(.), weight, 1))
m_sp_1 &lt;- map(
  alist(
    spur_y ~ dnorm(mu, sigma) ,
    mu &lt;- a + b1*spur_x, 
    a     ~ dnorm( 0, 100),
    b1    ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_sp_1))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;
]

---
## Spurious Models 2

.small[
.pull-left[

```r
m_sp_2 &lt;- map(
  alist(
    spur_y ~ dnorm(mu, sigma) ,
    mu &lt;- a + b1*weight + b2*spur_x, 
    a         ~ dnorm( 0, 100),
    c(b1, b2) ~ dnorm(0, 2), 
    sigma     ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_sp_2))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

In the presence of `weight`, `b2` is near 0; `spur_x` has no effect.

]
.pull-right[

```r
m_sp_3 &lt;- map(
  alist(
    spur_y ~ dnorm(mu, sigma) ,
    mu &lt;- a + b1*weight, 
    a     ~ dnorm( 0, 100),
    b1    ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_sp_3))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

Removing `spur_x` has minimal impact on `b1`, but precision increases.

]
]

---
## Multiple Causation

&lt;img src="https://clanfear.github.io/multivariate_lecture/img/xzy_masquerade.PNG" height=250 class="image-center"&gt;

Conditions of multiple (masked) causation:

* `\(X\)` directly impacts `\(Y\)` positively.
* `\(Z\)` directly impacts `\(Y\)` negatively.
* `\(X\)` and `\(Z\)` are correlated.
* If only `\(X\)` or `\(Z\)` is included, its effect is attenuated.

*This is also omitted variable bias*&lt;sup&gt;1

.pull-right[
.footnote[[1] In this case it appears to be a form of suppression?]
]

---
## Multiple Causation Models 1

.small[
.pull-left[

```r
m_sp_1 &lt;- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu &lt;- a + b1*area, 
    a     ~ dnorm( 0, 100),
    b1    ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std)
precis(m_sp_1)
```

```
##       Mean StdDev  5.5% 94.5%
## a     0.00   0.09 -0.15  0.15
## b1    0.02   0.09 -0.13  0.17
## sigma 1.00   0.07  0.89  1.10
```

We see no notable relationship between area and fox weight.

]
.pull-right[

```r
m_sp_2 &lt;- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu &lt;- a + b2*groupsize, 
    a     ~ dnorm( 0, 100),
    b2    ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std)
precis(m_sp_2)
```

```
##        Mean StdDev  5.5% 94.5%
## a      0.00   0.09 -0.15  0.15
## b2    -0.16   0.09 -0.31 -0.01
## sigma  0.98   0.06  0.88  1.09
```

We also see a minimal relationship for group size and fox weight.

]
]

---
## Multiple Causation Models 2

.small[

```r
m_sp_3 &lt;- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu &lt;- a + b1*area + b2*groupsize, 
    a         ~ dnorm( 0, 100),
    c(b1, b2) ~ dnorm(0, 2), 
    sigma     ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_sp_3))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;
]

* Net of area, group size has a negative association with weight.
* Net of group size, area has a positive assocation with weight.

*Residualizing each variable strengthens the relationship*

---
## Multicollinearity

&lt;img src="https://clanfear.github.io/multivariate_lecture/img/xzy_collinearity.PNG" height=250 class="image-center"&gt;

Conditions of multicollinearity:

* `\(X\)` and `\(Z\)` are highly correlated--they provide the same information.
* If `\(X\)` and `\(Z\)` are both included, estimates are uncertain.
* This redundancy doesn't hurt prediction--you get accurate posterior predictions.

*This is not bias, but weak identification.*

---
## Diagnosing Multicollinearity

.small[

```r
m_mc_1 &lt;- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu &lt;- a + b1*avgfood + b2*area, 
    a         ~ dnorm( 0, 100),
    c(b1, b2) ~ dnorm(0, 2), 
    sigma     ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_mc_1))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;

```r
cor(foxes_std)["avgfood", "area"]
```

```
## [1] 0.8831038
```
]

---
## Diagnosing Multicollinearity

.small[
.pull-left[

```r
m_mc_2 &lt;- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu &lt;- a + b1*area, 
    a     ~ dnorm( 0, 100),
    b1    ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_mc_2))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[

```r
m_mc_3 &lt;- map(
  alist(
    weight ~ dnorm(mu, sigma) ,
    mu &lt;- a + b2*avgfood, 
    a     ~ dnorm( 0, 100),
    b2    ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_mc_3))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;
]
]

&lt;br&gt;
&lt;br&gt;

*We get a more precise estimate... but near zero.*

---
## Post-Treatment Bias

&lt;img src="https://clanfear.github.io/multivariate_lecture/img/xzy_posttreatment.PNG" height=250 class="image-center"&gt;

Conditions of post-treatment bias:

* `\(X\)` impacts `\(Y\)` through `\(Z\)`--it is a mediator.
* `\(Z\)` provides much of same information as `\(X\)`.
* If `\(Z\)` is included, `\(B_2\)` absorbs some of `\(B_1\)`.

*This is included variable bias.*

---
## Post-Treatment Model 1

.small[

```r
foxes_std &lt;- foxes_std %&gt;% 
  mutate(., pt_z = rnorm(nrow(.), weight, 1),
            pt_y = rnorm(nrow(.), 0.4*pt_z + 0.1*weight, 0.25))
m_pt_1 &lt;- map(
  alist(
    pt_y ~ dnorm(mu, sigma) ,
    mu &lt;- a + b1*pt_z + b2*weight, 
    a         ~ dnorm( 0, 100),
    c(b1, b2) ~ dnorm(0, 2), 
    sigma     ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_pt_1))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;
]

---
## Post-Treatment Model 2

.small[
.pull-left[

```r
m_pt_2 &lt;- map(
  alist(
    pt_y ~ dnorm(mu, sigma) ,
    mu &lt;- a + b2*weight, 
    a     ~ dnorm( 0, 100),
    b2    ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_pt_2))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[

```r
m_pt_3 &lt;- map(
  alist(
    pt_y ~ dnorm(mu, sigma) ,
    mu &lt;- a + b2*pt_z, 
    a     ~ dnorm( 0, 100),
    b2    ~ dnorm(0, 2), 
    sigma ~ dunif(0, 10)
  ), data=foxes_std)
plot(precis(m_pt_3))
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;
]
]
---
## Post-Treatment Model 3

.smaller[
.pull-left[

```r
wt_seq &lt;- seq(-2.5,2.5,0.01)
mu &lt;- link(m_pt_2, data=list(weight = wt_seq))
mu.mean &lt;- apply(mu, 2, mean)
mu.HPDI &lt;- apply(mu, 2, HPDI, prob=0.95)
weight.sim &lt;- sim(m_pt_2, data=list(weight = wt_seq))
weight.HPDI &lt;- apply(weight.sim, 2, HPDI, prob=0.95)
plot(pt_y ~ weight, data=foxes_std)
lines( wt_seq, mu.mean)
shade( mu.HPDI, wt_seq)
shade( weight.HPDI, wt_seq)
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;
]
.pull-right[

```r
pt_z_seq &lt;- seq(-2.5,2.5,0.01)
mu &lt;- link(m_pt_3, data=list(pt_z = pt_z_seq))
mu.mean &lt;- apply(mu, 2, mean)
mu.HPDI &lt;- apply(mu, 2, HPDI, prob=0.95)
pt_z.sim &lt;- sim(m_pt_3, data=list(pt_z = pt_z_seq))
pt_z.HPDI &lt;- apply(pt_z.sim, 2, HPDI, prob=0.95)
plot(pt_y ~ pt_z, data=foxes_std)
lines( pt_z_seq, mu.mean)
shade( mu.HPDI, pt_z_seq)
shade( pt_z.HPDI, pt_z_seq)
```

&lt;img src="multivariate_models_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;
]
]
---
# Key Takeaways

* Aside from multicollinearity, it is hard to distinguish between different relationships.

* Need a clear causal model for relationships to make sense of it.

* Including additional variables doesn't always improve the model.

* Additional variables usually improve in-sample fit.

---
# An Optimizer Example

119 iterations of a Nelder-Mead optimizer on OLS.

Start Values: `\(\alpha = 1\)`, `\(\beta = 1\)`

Squares depict **squared errors**.

Minimizing the **sum squared errors** is the objective of the mean loss function in a linear regression.

&lt;img src="http://www.jessefagan.com/content/images/2019/10/advertising_album_sales_regression_animation_06.gif"&gt;

Source: Jesse Fagan, [R code available here](https://gist.github.com/jfaganUK/0f38d414d9a132d029c366ac310a2a06).
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "tomorrow-night-bright",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
